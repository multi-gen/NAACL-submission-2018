{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beam_size = 10\n",
    "# templates_dump_location = './eo_with_surf_forms_templates.p'\n",
    "templates_dump_location = './ar_without_surf_forms_templates.p'\n",
    "# Whether to sample from sentences with surface form tuples and property placeholders OR with pure text.\n",
    "# summaries_type = 'summary_with_surf_forms_and_types'\n",
    "summaries_type = 'original_summary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading trained $n$-gram model along with the original dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel('ar_without_surf_forms.klm')\n",
    "dataset_file_location = '../Datasets/ar/Dataset/with-Surface-Forms/splitDataset_with_targets.p'\n",
    "with open(dataset_file_location, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_len = 0\n",
    "token2id = {}\n",
    "id2token = {}\n",
    "\n",
    "for i in range(0, len(dataset['train'][summaries_type])):\n",
    "    tempSummary = dataset['train'][summaries_type][i].encode('utf-8').split()\n",
    "    for token in tempSummary:\n",
    "        if token not in token2id:\n",
    "            vocab_len += 1\n",
    "            token2id[token] = vocab_len\n",
    "            id2token[vocab_len] = token\n",
    "vocab_len += 1\n",
    "token2id['</s>'] = vocab_len\n",
    "id2token[vocab_len] = '</s>'\n",
    "\n",
    "# Make sure that sequences are absolved from start- and end-of-sequence tokens.\n",
    "# These are not handled very well by this model.\n",
    "assert('<start>' not in token2id)\n",
    "assert('<end>' not in token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam-search decoding on the trained $n$-gram model\n",
    "We initialise the beams with the $n$ most probable words given the `<s>` token (i.e. `<start>` token in KenLM Language Model Toolkit)\n",
    "\n",
    "More information at: https://kheafield.com/code/kenlm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[423458     23   1197    487   3478    639    421     10      7     43]\n",
      "[ 852805  864784    1078  848042    6411  423502 1694807     784  847022\n",
      "  423553]\n",
      "[1271582      95 2545349 2541879 3812537 1693845 2964375  847080  437077\n",
      "     120]\n",
      "[3813604    1706 3394202 3813713 3387768 2541328 1693846 1271165  847703\n",
      "     121]\n",
      "[1270519  847997 2964238 2541725  861699     504 2117392  846968 1270518\n",
      "  423585]\n",
      "[ 865303  856119  854492 3387682    2804 2964333 2117305  864152  423469\n",
      "     786]\n",
      "[3811141 2975390 3396874 2541140 1700102 2120100 1270763  846928  423601\n",
      "     386]\n",
      "[1694222 1277951    3619  847304   19845    6889    2628    4774     787\n",
      "   10919]\n",
      "[3811181 3398849 2965001 2118083 2540804 1694624 1278554  851664  424247\n",
      "     788]\n",
      "[  10920  436005 3387682  423509     400   19710 2134529      50 1271165\n",
      "  847706]\n",
      "[2565451 2129236 3387800 2964312    6168      50 1693964  431033 1270387\n",
      "  859251]\n",
      "[3811259 3387800 2964274 1698610 2555194 2117423 1270438  863225  423558\n",
      "     128]\n",
      "[3811192 3387733 2964275 2551454 2540753 1693897 1270439  847046  437899\n",
      "      61]\n",
      "[2599823 2969594 3387734 2964275 2117423 1693898  846979  846917      62]\n",
      "[ 267174   53271   34674   27153   17004  428840 1270438  423521]\n",
      "[2964343  224861 1270507 1693966  854444   23109   24350  423521]\n",
      "[2964344  847048  423589  151129   60389 2117426 1693967    7256]\n",
      "[2964274     130 1693966  423520 1270507 2540885  846979 2117426]\n",
      "[2964345 2540885 2117426      61 1693968  846979 1270508  423591]\n",
      "[ 423520 2964346 2540815 2118086 1693968 1270510  847050     133]\n",
      "[2964345 2540888 2117427 1694628 1270510  847052  423592     134]\n",
      "[2964346 2964212 1693969 2117428 1270511 1270376  423593  423458]\n",
      "[1693970 1271047  847052  846917  423458]\n",
      "[1270376  423915  423458]\n",
      "[423458]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "sentences_prob = []\n",
    "candidates = []\n",
    "num_active_beams = beam_size\n",
    "\n",
    "\n",
    "beam_probabilities = np.zeros(vocab_len) \n",
    "for token in token2id:\n",
    "    tempCandidate = ' '.join([] + [token]) \n",
    "    beam_probabilities[token2id[token] - 1] = model.score(tempCandidate, eos = False)\n",
    "indices = np.argsort(beam_probabilities)[-num_active_beams:]\n",
    "print indices\n",
    "for j in range(beam_size - 1, -1, -1):\n",
    "    candidates.append([id2token[indices[j] + 1]])\n",
    "\n",
    "while num_active_beams > 0:\n",
    "    beam_probabilities = np.zeros(num_active_beams * vocab_len)\n",
    "    beam_probabilities.fill(np.NINF)\n",
    "    for s in range(0, num_active_beams):\n",
    "        for token in token2id:\n",
    "            tempCandidate = ' '.join(candidates[s] + [token])\n",
    "            beam_probabilities[s * vocab_len + token2id[token] - 1] = model.score(tempCandidate, eos = False)\n",
    "        \n",
    "    indices = np.argsort(beam_probabilities)[-num_active_beams:]\n",
    "    print indices\n",
    "    cloned_candidates = copy.deepcopy(candidates)\n",
    "    completed_beams_counter = 0\n",
    "    candidates = []\n",
    "    for j in range(num_active_beams - 1, -1, -1):\n",
    "        \n",
    "        candidates.append([])\n",
    "        candidates[-1] = copy.deepcopy(cloned_candidates[indices[j] / vocab_len])\n",
    "        \n",
    "        candidates[-1] += [id2token[indices[j] % vocab_len + 1]]\n",
    "        if id2token[indices[j] % vocab_len + 1] == '</s>':\n",
    "            completed_beams_counter += 1\n",
    "            sentences.append(candidates[-1])\n",
    "            sentences_prob.append(beam_probabilities[indices[j]])\n",
    "            candidates.pop(-1)\n",
    "    num_active_beams -= completed_beams_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purely-greedy decoding on the trained $n$-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentences = [[]]\n",
    "# sentences_prob = [np.NINF]\n",
    "# selectedToken = ''\n",
    "\n",
    "# while selectedToken != '</s>':\n",
    "#     max_prob = np.NINF\n",
    "#     for token in token2id:\n",
    "#         tempCandidate = ' '.join(sentences[0] + [token])\n",
    "#         sentences_prob[-1] = tempScore\n",
    "#         tempScore = model.score(tempCandidate, eos = False)\n",
    "#         if tempScore > max_prob:\n",
    "#             selectedToken = token\n",
    "#             max_prob = tempScore\n",
    "#     sentences[0].append(selectedToken)\n",
    "\n",
    "#     print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x84\\xd9\\x88\\xd9\\x83', '\\xd9\\x81\\xd9\\x8a', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd8\\xaa\\xd9\\x84', '\\xd8\\xa3\\xd8\\xa8\\xd9\\x8a\\xd8\\xb6', '\\xd9\\x81\\xd9\\x8a', '\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x82\\xd8\\xa9.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x82\\xd8\\xa9', '\\xd9\\x81\\xd9\\x8a', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x82\\xd8\\xa9', '\\xd9\\x81\\xd9\\x8a', '\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb1\\xd9\\x82\\xd8\\xa9.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd9\\x85\\xd9\\x86\\xd8\\xa8\\xd8\\xac', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd9\\x85\\xd9\\x86\\xd8\\xa8\\xd8\\xac\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '195', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x86', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd9\\x85\\xd9\\x86\\xd8\\xa8\\xd8\\xac', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd9\\x85\\xd9\\x86\\xd8\\xa8\\xd8\\xac\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '282', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x86', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd9\\x85\\xd9\\x86\\xd8\\xa8\\xd8\\xac', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd9\\x85\\xd9\\x86\\xd8\\xa8\\xd8\\xac\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '846', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x86', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd8\\xac\\xd8\\xa8\\xd9\\x84', '\\xd8\\xb3\\xd9\\x85\\xd8\\xb9\\xd8\\xa7\\xd9\\x86', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xaa\\xd9\\x84', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb6\\xd9\\x85\\xd8\\xa7\\xd9\\x86\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '332', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x86', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd8\\xac\\xd8\\xa8\\xd9\\x84', '\\xd8\\xb3\\xd9\\x85\\xd8\\xb9\\xd8\\xa7\\xd9\\x86', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xaa\\xd9\\x84', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb6\\xd9\\x85\\xd8\\xa7\\xd9\\x86\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '295', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x86', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd8\\xb9\\xd9\\x8a\\xd9\\x86', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd8\\xb9\\xd9\\x8a\\xd9\\x86', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '76', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x86', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd8\\xb9\\xd9\\x8a\\xd9\\x86', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd8\\xb9\\xd9\\x8a\\xd9\\x86', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '649', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd9\\x8a\\xd9\\x85\\xd9\\x86', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004.', '</s>']\n",
      "['\\xd9\\x82\\xd8\\xb1\\xd9\\x8a\\xd8\\xa9', '\\xd8\\xb3\\xd9\\x88\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa9', '\\xd8\\xaa\\xd8\\xaa\\xd8\\xa8\\xd8\\xb9', '\\xd8\\xa5\\xd8\\xaf\\xd8\\xa7\\xd8\\xb1\\xd9\\x8a\\xd9\\x91\\xd8\\xa7\\xd9\\x8b', '\\xd9\\x84\\xd9\\x85\\xd8\\xad\\xd8\\xa7\\xd9\\x81\\xd8\\xb8\\xd8\\xa9', '\\xd8\\xad\\xd9\\x84\\xd8\\xa8', '\\xd9\\x85\\xd9\\x86\\xd8\\xb7\\xd9\\x82\\xd8\\xa9', '\\xd8\\xb9\\xd9\\x8a\\xd9\\x86', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8', '\\xd9\\x86\\xd8\\xa7\\xd8\\xad\\xd9\\x8a\\xd8\\xa9', '\\xd9\\x85\\xd8\\xb1\\xd9\\x83\\xd8\\xb2', '\\xd8\\xb9\\xd9\\x8a\\xd9\\x86', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd8\\x8c', '\\xd8\\xa8\\xd9\\x84\\xd8\\xba', '\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x87\\xd8\\xa7', '1,142', '\\xd9\\x86\\xd8\\xb3\\xd9\\x85\\xd8\\xa9', '\\xd8\\xad\\xd8\\xb3\\xd8\\xa8', '\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd8\\xaf\\xd8\\xa7\\xd8\\xaf', '\\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x83\\xd8\\xa7\\xd9\\x86\\xd9\\x8a', '\\xd9\\x84\\xd8\\xb9\\xd8\\xa7\\xd9\\x85', '2004', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قرية سورية تتبع ناحية سلوك في منطقة تل أبيض في محافظة الرقة. -4.30183410645\n",
      "قرية سورية تتبع ناحية مركز الرقة في منطقة مركز الرقة في محافظة الرقة. -4.10472583771\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة منبج ناحية مركز منبج، بلغ تعداد سكانها 195 نسمة حسب تعداد اليمن لعام 2004. -6.04410171509\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة منبج ناحية مركز منبج، بلغ تعداد سكانها 282 نسمة حسب تعداد اليمن لعام 2004. -6.12608671188\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة منبج ناحية مركز منبج، بلغ تعداد سكانها 846 نسمة حسب تعداد اليمن لعام 2004. -6.34612083435\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة جبل سمعان ناحية تل الضمان، بلغ تعداد سكانها 332 نسمة حسب تعداد اليمن لعام 2004. -6.11217737198\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة جبل سمعان ناحية تل الضمان، بلغ تعداد سكانها 295 نسمة حسب تعداد اليمن لعام 2004. -6.14283323288\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة عين العرب ناحية مركز عين العرب، بلغ تعداد سكانها 76 نسمة حسب تعداد اليمن لعام 2004. -6.26259183884\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة عين العرب ناحية مركز عين العرب، بلغ تعداد سكانها 649 نسمة حسب تعداد اليمن لعام 2004. -6.38466739655\n",
      "قرية سوريّة تتبع إداريّاً لمحافظة حلب منطقة عين العرب ناحية مركز عين العرب، بلغ تعداد سكانها 1,142 نسمة حسب التعداد السكاني لعام 2004 . -6.36081552505\n"
     ]
    }
   ],
   "source": [
    "for s in range(0, len(sentences)):\n",
    "    assert(sentences[s][-1] == '</s>')\n",
    "    sentences[s] = ' '.join(sentences[s][:-1]).decode('utf-8')\n",
    "    print sentences[s], sentences_prob[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing probabilities\n",
    "KenLM scores are $log_{10}$ probabilities; we compute the actual ones (i.e. $0 \\leq p_i \\leq 1$).\n",
    "We normalise as follows:\n",
    "\\begin{equation}\n",
    "\\widetilde{p_i} = \\frac{p_i}{\\sum_i p_i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nominator = np.power(10, np.asarray(sentences_prob))\n",
    "denominator = np.power(10, np.asarray(sentences_prob)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_distribution = nominator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.37392506  0.58869839  0.00676888  0.00560444  0.00337674  0.00578684\n",
      "  0.00539244  0.00409286  0.00308995  0.0032644 ]\n"
     ]
    }
   ],
   "source": [
    "print prob_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving everything in a dictionary\n",
    "```python\n",
    "{'sentences': type(list), 'prob_distribution': type(numpy.ndarray)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(templates_dump_location, 'wb') as f:\n",
    "    pickle.dump({'sentences': sentences, 'prob_distribution': prob_distribution}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
